{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Multi-Agent System to a Single-Model Baseline for Code Generation\n",
    "\n",
    "This notebook presents a comparative study evaluating the performance of a multi-agent system for automated code generation against a single large language model (LLM) baseline. The multi-agent system, designed with specialized agents for tasks such as preprocessing, retrieval, planning, coding, and debugging, aims to tackle complex coding challenges more effectively than a monolithic single-model approach.\n",
    "\n",
    "The objective of this study is to quantify the difference in performance between the two approaches across a set of coding problems. We will evaluate performance based on metrics such as the success rate in generating correct code, the time taken to produce a solution, and potentially other relevant factors like the number of iterations or debugging steps.\n",
    "\n",
    "This notebook will cover:\n",
    "\n",
    "1.  **Setup:** Loading necessary libraries, configuring the environment, and preparing the coding problems for evaluation.\n",
    "2.  **Baseline (Single Model):** Implementing and running the single-model approach on the chosen coding problems.\n",
    "3.  **Multi-Agent System:** Integrating and running the existing multi-agent system within the notebook environment.\n",
    "4.  **Evaluation:** Running both approaches on the same set of problems and collecting performance data.\n",
    "5.  **Analysis:** Comparing the collected data to draw conclusions about the effectiveness of each approach.\n",
    "6.  **Results and Discussion:** Presenting the findings and discussing the implications of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/twanh/workspace/thesis/thesis-advent-of-agents/src/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "# Append the models path in order to import the models\n",
    "PROJECT_ROOT = os.path.join(os.getcwd(), 'src/')\n",
    "print(PROJECT_ROOT)\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Load env variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Set log level INFO\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models from the system\n",
    "from models.base_model import BaseLanguageModel\n",
    "from models.gemini_model import GeminiLanguageModel\n",
    "from models.openai_model import OpenAILanguageModel\n",
    "from models.deepseek_model import DeepseekLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load puzzles and input/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correct paths\n",
    "test_data_folder = os.path.join(PROJECT_ROOT, '..', 'experiments', 'test_data')\n",
    "puzzles_folder = os.path.join(test_data_folder, 'puzzles/')\n",
    "input_output_file = os.path.join(test_data_folder, 'answers2024.json')\n",
    "puzzle_files = [os.path.join(puzzles_folder, f) for f in os.listdir(puzzles_folder) if os.path.isfile(os.path.join(puzzles_folder, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "# Create a datastructure were we can get by day\n",
    "json_data = {}\n",
    "with open(input_output_file, 'r') as f:\n",
    "    json_data = {item['day']: item for item in json.load(f)}\n",
    "\n",
    "puzzle_data = []\n",
    "for file_path in puzzle_files:\n",
    "    # Get the day of the puzzle file\n",
    "    file_name = os.path.basename(file_path)\n",
    "    day_str = file_name.split('_')[-1].split('.')[0]\n",
    "    day = int(day_str)\n",
    "\n",
    "    if day in json_data:\n",
    "        with open(file_path, 'r') as f:\n",
    "            puzzle_description = f.read()\n",
    "\n",
    "        puzzle_info = {\n",
    "            \"year\": json_data[day]['year'],\n",
    "            \"day\": day,\n",
    "            \"description\": puzzle_description,\n",
    "            \"input\": json_data[day]['input'],\n",
    "            \"expected_output\": json_data[day]['part1']\n",
    "        }\n",
    "\n",
    "        puzzle_data.append(puzzle_info)\n",
    "\n",
    "# Sort by day\n",
    "puzzle_data.sort(key=lambda x: x['day'])\n",
    "print(len(puzzle_data)) # should be 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configurations\n",
    "\n",
    "Create the configurations for the baseline and the system to use. Since the advent of agents system can use multiple models for each agent, the model used for each agent will be the same as the single model used for the baseline.\n",
    "\n",
    "The models that will be tested are:\n",
    "\n",
    "<!-- TODO: Update model list -->\n",
    "- Gemini\n",
    "- OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'baseline': <models.gemini_model.GeminiLanguageModel object at 0x701702110590>, 'system': {'preprocess': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f46c30>, 'retreival': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f476e0>, 'planning': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f781d0>, 'coding': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f78c80>, 'debugging': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f79760>}}, {'baseline': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f7a210>, 'system': {'preprocess': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f7acc0>, 'retreival': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f7b770>, 'planning': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f88260>, 'coding': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f88ce0>, 'debugging': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f89730>}}, {'baseline': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f8a1b0>, 'system': {'preprocess': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f8adb0>, 'retreival': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f8b890>, 'planning': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f983e0>, 'coding': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f98ef0>, 'debugging': <models.gemini_model.GeminiLanguageModel object at 0x7016e3f99a00>}}, {'baseline': <models.openai_model.OpenAILanguageModel object at 0x7016e3f9a4e0>, 'system': {'preprocess': <models.openai_model.OpenAILanguageModel object at 0x7016e3fac6e0>, 'retreival': <models.openai_model.OpenAILanguageModel object at 0x7016e3faed20>, 'planning': <models.openai_model.OpenAILanguageModel object at 0x7016e3fc53a0>, 'coding': <models.openai_model.OpenAILanguageModel object at 0x7016e3fc7a40>, 'debugging': <models.openai_model.OpenAILanguageModel object at 0x7016e3fe2030>}}]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from main import _get_model\n",
    "\n",
    "models_to_test = ('gemini-2.0-flash', 'gemini-2.5-flash-preview-04-17', 'gemini-2.5-pro-preview-05-06' ,'gpt-4.1-mini')\n",
    "\n",
    "configs = []\n",
    "for model in models_to_test:\n",
    "    configs.append({\n",
    "        'baseline': _get_model(model),\n",
    "        'system': {\n",
    "            'preprocess': _get_model(model),\n",
    "            'retreival': _get_model(model),\n",
    "            'planning': _get_model(model),\n",
    "            'coding': _get_model(model),\n",
    "            'debugging': _get_model(model),\n",
    "        }\n",
    "    }) \n",
    "print(configs)\n",
    "print(len(configs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline prompt\n",
    "\n",
    "The prompt is based on the prompt the advent of agents system uses. However the baseline has no acess to the information the other agents provide. So there is only the `full_description` that provides the model with the puzzle. \n",
    "The steps in the prompt are the same expect the first step is removed, which was to analyze the generated plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_PROMPT = \"\"\"\n",
    "# Advent of Code Implementation Agent\n",
    "\n",
    "You are an expert coding agent specializing in implementing solutions for Advent of Code puzzles.\n",
    "Your task is to convert a detailed solution plan into clean, efficient, and correct Python code that solves the given problem.\n",
    "You excel at translating algorithmic plans into precise implementations.\n",
    "\n",
    "\n",
    "It will be provided as the following JSON\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"full_description\": \"The full description of the problem (string)\",\n",
    "}}\n",
    "```\n",
    "\n",
    "## YOUR RESPONSIBILITIES\n",
    "\n",
    "Your primary goal is to produce a complete, correct, and efficient Python implementation that:\n",
    "\n",
    "1. Correctly solves both the provided examples and will work for the actual puzzle input\n",
    "2. Follows good software engineering practices\n",
    "3. Includes appropriate comments and documentation\n",
    "4. Handles edge cases and potential errors\n",
    "5. Is executable via command line as: `python3 [program].py [puzzleinputfile]`\n",
    "\n",
    "## IMPLEMENTATION PROCESS\n",
    "\n",
    "Follow these steps meticulously:\n",
    "\n",
    "-----------------------------------------\n",
    "STEP 1. Design Your Code Structure\n",
    "-----------------------------------------\n",
    "\n",
    "- Create a clear, modular structure with well-named functions matching the plan's major steps\n",
    "- Define appropriate data structures with explicit type hints\n",
    "- Plan your function signatures and interfaces before implementation\n",
    "- Use the keywords and underlying concepts to think about what algoritms to use to solve the problems.\n",
    "\n",
    "-----------------------------------------\n",
    "STEP 2. Implement Core Logic\n",
    "-----------------------------------------\n",
    "\n",
    "- Write robust implementations of all algorithms described in the plan\n",
    "- Include detailed comments explaining complex logic\n",
    "- Follow Python best practices (PEP 8, appropriate naming conventions)\n",
    "- Use type hints throughout your code\n",
    "\n",
    "\n",
    "-----------------------------------------\n",
    "STEP 3. Handle Edge Cases Explicitly\n",
    "-----------------------------------------\n",
    "\n",
    "- Add specific code to handle all edge cases mentioned in the plan\n",
    "- Anticipate and handle additional edge cases common in Advent of Code:\n",
    "  - Empty input\n",
    "  - Boundary conditions (min/max values)\n",
    "- Use the test cases to reason about your code and make sure it would solve the test cases correctly\n",
    "\n",
    "----------------------------------------\n",
    "STEP 4. Test Against Examples\n",
    "----------------------------------------\n",
    "\n",
    "- Include code that runs and validates against all provided examples\n",
    "- Add assertions to verify intermediate results match expected values\n",
    "- Print debugging information that would help diagnose issues to STDERR\n",
    "    - STDOUT can only be used to print the final result.\n",
    "\n",
    "----------------------------------------\n",
    "STEP 5. Optimize If Necessary\n",
    "-----------------------------------------\n",
    "\n",
    "- Review your solution for performance bottlenecks\n",
    "- Apply optimizations where appropriate, explaining your choices\n",
    "- Ensure the solution will scale to handle the full problem input\n",
    "\n",
    "-----------------------------------------\n",
    "STEP 6. Finalize Solution\n",
    "-----------------------------------------\n",
    "\n",
    "- Ensure your code has a clear entry point (typically a `main()` function)\n",
    "- Include code to read from the puzzle input file specified as a command-line argument\n",
    "- Make sure that your code follows the proper structure as documented (example code template) below.\n",
    "- Add a brief summary comment at the top explaining the approach\n",
    "- Verify all functions have appropriate docstrings\n",
    "\n",
    "\n",
    "-----------------------------------------\n",
    "OUTPUT FORMAT\n",
    "-----------------------------------------\n",
    "Your response must be a valid JSON object with the following structure:\n",
    "\n",
    "The generated code should be provided as the value of the code key in the JSON object. Ensure that the code is properly escaped to be a valid JSON string. This means that any double quotes within the code should be escaped with a backslash (\\\"), and newlines should be represented as \\\\n\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"code\": \"Complete Python code as a string with all necessary formatting. MAKE SURE THAT THIS IS VALID JSON\"\n",
    "}}\n",
    "```\n",
    "\n",
    "\n",
    "## EXAMPLE CODE TEMPLATE\n",
    "\n",
    "```python\n",
    "\\\"\\\"\\\"\n",
    "Advent of Code [Year] Day [Number]: [Title]\n",
    "Solution implementation based on the provided plan.\n",
    "\n",
    "Usage: python3 solution.py [input_file]\n",
    "\\\"\\\"\\\"\n",
    "from typing import List, Dict, Tuple, Set, Optional\n",
    "import sys\n",
    "from collections import defaultdict, deque\n",
    "import re\n",
    "# Import other necessary libraries\n",
    "\n",
    "def parse_input(input_file: str) -> [appropriate_return_type]:\n",
    "    \"\\\"\\\"Parse the puzzle input from file into appropriate data structures.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to the input file\n",
    "\n",
    "    Returns:\n",
    "        [Description of return value]\n",
    "    \\\"\\\"\\\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        # Process file content\n",
    "        pass\n",
    "    # Implementation...\n",
    "\n",
    "def solve_part_one(parsed_data: [type]) -> [type]:\n",
    "    \\\"\\\"\\\"Solve part one of the puzzle.\n",
    "\n",
    "    Args:\n",
    "        parsed_data: Processed input data\n",
    "\n",
    "    Returns:\n",
    "        Solution for part one\n",
    "    \\\"\\\"\\\"\n",
    "    # Implementation...\n",
    "\n",
    "def main():\n",
    "    # Check command line arguments\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python3 solution.py [input_file]\")\n",
    "        return\n",
    "\n",
    "    input_file = sys.argv[1]\n",
    "\n",
    "    # Parse input\n",
    "    parsed_data = parse_input(input_file)\n",
    "\n",
    "    # Solve part one\n",
    "    part_one_solution = solve_part_one(parsed_data)\n",
    "    # ONLY PRINT THE RESULT, NO OTHER TEXT\n",
    "    print(part_one_solution)\n",
    "\n",
    "    # Test with examples (if available)\n",
    "    # [Example testing code]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "Remember to follow the plan closely while filling in implementation details that the planner may have omitted. Your goal is to bridge the gap between algorithmic description and working code.\n",
    "\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "Your input is:\n",
    "\n",
    "{json_input}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import extract_json_from_markdown\n",
    "from agents.debugging_agent import DebuggingAgent\n",
    "from utils.util_types import TestCase\n",
    "\n",
    "def run_and_test_baseline(puzzle: str, puzzle_input: str, expected_output: str, model: BaseLanguageModel) -> bool:\n",
    "\n",
    "    # Create the prompts\n",
    "    json_inp = json.dumps({'full_description': puzzle})\n",
    "    prompt = BASELINE_PROMPT.format(json_input=json_inp)\n",
    "    # Prompt the model\n",
    "    resp = model.prompt(prompt)\n",
    "\n",
    "    # Extract the solution\n",
    "    try:\n",
    "        code = json.loads(extract_json_from_markdown(resp)[0]).get('code')\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed because decode\")\n",
    "        # TODO: Add retrying?\n",
    "        return False\n",
    "\n",
    "    # Use debugging agent to test the final solution\n",
    "    dba = DebuggingAgent(\n",
    "        'debugging',\n",
    "        model=model,\n",
    "        expected_output=expected_output,\n",
    "        puzzle_input=puzzle_input,\n",
    "    )\n",
    "    run_result = dba._run_test(\n",
    "        code,\n",
    "        TestCase(\n",
    "            input_=puzzle_input,\n",
    "            expected_output=expected_output,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return run_result.success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gemini-2.0-flash\n",
      "Running day 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:28:45.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:28:45.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:28:45.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 1646452=1646452\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:28:49.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:28:49.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:28:49.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 524=524\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 3\n",
      "Failed because decode\n",
      "Not solved\n",
      "Running day 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:28:58.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:28:58.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:28:58.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 2464=2464\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:29:03.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:03.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:03.263\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m357\u001b[0m - \u001b[33m\u001b[1mTest case was not successful\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:03.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m358\u001b[0m - \u001b[1mGot: 5997, expected: 5391\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not solved\n",
      "Running day 6\n",
      "Failed because decode\n",
      "Not solved\n",
      "Running day 7\n",
      "Failed because decode\n",
      "Not solved\n",
      "Running day 8\n",
      "Failed because decode\n",
      "Not solved\n",
      "Running day 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:29:23.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:23.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:28.590\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m324\u001b[0m - \u001b[33m\u001b[1mTimeout for running code expired.\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:28.591\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m357\u001b[0m - \u001b[33m\u001b[1mTest case was not successful\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:28.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m358\u001b[0m - \u001b[1mGot: None, expected: 6279058075753\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not solved\n",
      "Running day 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:29:33.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:33.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:33.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 459=459\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:29:37.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:37.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:38.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 193899=193899\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:29:44.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:44.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:44.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 1449902=1449902\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:29:51.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:51.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:51.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 29517=29517\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:29:58.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:58.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:29:58.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 230461440=230461440\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:30:05.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:05.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:06.073\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m357\u001b[0m - \u001b[33m\u001b[1mTest case was not successful\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:06.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m358\u001b[0m - \u001b[1mGot: 1453784, expected: 1478649\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not solved\n",
      "Running day 16\n",
      "Failed because decode\n",
      "Not solved\n",
      "Running day 17\n",
      "Failed because decode\n",
      "Not solved\n",
      "Running day 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:30:22.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:22.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:22.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 334=334\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 19\n",
      "Failed because decode\n",
      "Not solved\n",
      "Running day 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:30:36.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:36.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:41.353\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m324\u001b[0m - \u001b[33m\u001b[1mTimeout for running code expired.\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:41.354\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m357\u001b[0m - \u001b[33m\u001b[1mTest case was not successful\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:41.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m358\u001b[0m - \u001b[1mGot: None, expected: 1415\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not solved\n",
      "Running day 21\n",
      "Failed because decode\n",
      "Not solved\n",
      "Running day 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:30:52.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:52.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:53.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 13234715490=13234715490\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:30:58.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:30:58.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:31:03.738\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m324\u001b[0m - \u001b[33m\u001b[1mTimeout for running code expired.\u001b[0m\n",
      "\u001b[32m2025-05-19 11:31:03.739\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m357\u001b[0m - \u001b[33m\u001b[1mTest case was not successful\u001b[0m\n",
      "\u001b[32m2025-05-19 11:31:03.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m358\u001b[0m - \u001b[1mGot: None, expected: 1485\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not solved\n",
      "Running day 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:31:11.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:31:11.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:31:11.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m345\u001b[0m - \u001b[1mTest case is successful 54715147844840=54715147844840\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved!\n",
      "Running day 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 11:31:17.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m337\u001b[0m - \u001b[1mRunning code with test case\u001b[0m\n",
      "\u001b[32m2025-05-19 11:31:17.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_code\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mRunning code\u001b[0m\n",
      "\u001b[32m2025-05-19 11:31:17.433\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m357\u001b[0m - \u001b[33m\u001b[1mTest case was not successful\u001b[0m\n",
      "\u001b[32m2025-05-19 11:31:17.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36magents.debugging_agent\u001b[0m:\u001b[36m_run_test\u001b[0m:\u001b[36m358\u001b[0m - \u001b[1mGot: 22250, expected: 2900\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not solved\n",
      "Solved: 11/25\n"
     ]
    }
   ],
   "source": [
    "baseline_results = {}\n",
    "total = 0\n",
    "solved = 0\n",
    "# TODO: remove slice when running full tests\n",
    "for config in configs[:1]:\n",
    "    baseline_model = config['baseline']\n",
    "    baseline_results[baseline_model.model_name] = {}\n",
    "    print(f\"Using model: {baseline_model.model_name}\")\n",
    "    for puzzle in puzzle_data:\n",
    "        total += 1\n",
    "        print(f\"Running day {puzzle['day']}\")\n",
    "        success = run_and_test_baseline(\n",
    "            puzzle=puzzle['description'],\n",
    "            puzzle_input=puzzle['input'],\n",
    "            expected_output=str(puzzle['expected_output']),\n",
    "            model=baseline_model\n",
    "        )\n",
    "        baseline_results[baseline_model.model_name][puzzle['day']] = success\n",
    "        if success:\n",
    "            print(\"Solved!\")\n",
    "            solved += 1\n",
    "        else:\n",
    "            print(\"Not solved\")\n",
    "\n",
    "print(f\"Solved: {solved}/{total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
